{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import pytesseract\n",
    "\n",
    "# Open the image and apply EXIF orientation if necessary\n",
    "image = Image.open(\"/Users/aaryas127/Documents/GitHub/pharmapp/pharmapp-backend/uploads/IMG_8512.jpeg\")\n",
    "image = ImageOps.exif_transpose(image)\n",
    "\n",
    "# Extract all text from the image\n",
    "full_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Find the position of \"Dr.\" in the text\n",
    "start_index = full_text.find(\"Dr.\")\n",
    "\n",
    "# Check if \"Dr.\" was found, and if so, extract text from that position onward\n",
    "if start_index != -1:\n",
    "    text = full_text[start_index:]\n",
    "    print(\"Extracted Text Starting from 'Dr.':\\n\", text)\n",
    "else:\n",
    "    print(\"The keyword 'Dr.' was not found in the image.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the Flan-T5 model with GPU support if available, or use CPU\n",
    "flan_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", device=-1)\n",
    "\n",
    "# Updated prompt with added structure guidance and example format\n",
    "text_input = f\"\"\"\n",
    "Please extract and organize the information from the following medical text into a structured format with specific sections. Use bullet points and organize under the following headers:\n",
    "\n",
    "1. **Doctor's Information**\n",
    "   - Name:\n",
    "   - Clinic:\n",
    "   - Address:\n",
    "   - Practitioner Number:\n",
    "   - Contact Info (Phone and Fax):\n",
    "\n",
    "2. **Patient Details**\n",
    "   - Name:\n",
    "   - Date of Birth (DOB):\n",
    "   - Address:\n",
    "   - Contact Number:\n",
    "   - Health Insurance Number:\n",
    "\n",
    "3. **Prescription**\n",
    "   - Medication:\n",
    "   - Dosage Instructions:\n",
    "   - Quantity:\n",
    "   - Repeats:\n",
    "\n",
    "4. **Document Metadata**\n",
    "   - Written Date:\n",
    "   - Created By:\n",
    "\n",
    "Medical Text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Return the output with this structure, clearly formatted and organized.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the structured response\n",
    "response = flan_pipeline([text_input], max_length=512, do_sample=False)\n",
    "\n",
    "# Print the structured output\n",
    "print(\"Structured Output:\\n\", response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "  base_url=\"https://integrate.api.nvidia.com/v1\",  # You can change this to OpenAI's URL if needed\n",
    "  api_key=\"\"\n",
    ")\n",
    "\n",
    "# Define the prompt, with the variable 'text' inserted\n",
    "prompt = f\"\"\"\n",
    "Please extract the following details from the medical text and format them into distinct sections:\n",
    "\n",
    "**Doctor's Information**\n",
    "- Name\n",
    "- Clinic\n",
    "- Address\n",
    "- Practitioner Number\n",
    "- Contact Info\n",
    "\n",
    "**Patient Details**\n",
    "- Name\n",
    "- DOB\n",
    "- Address\n",
    "- Contact Info\n",
    "- Health Insurance Number\n",
    "\n",
    "**Prescription**\n",
    "- Medication\n",
    "- Dosage Instructions\n",
    "- Quantity\n",
    "- Repeats\n",
    "\n",
    "Medical Text:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "# Request completion from OpenAI's model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"meta/llama-3.2-3b-instruct\",  # You can switch this model to one compatible with OpenAI if needed\n",
    "  messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "# Collect and print the response as it's streamed\n",
    "for chunk in completion:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        output = chunk.choices[0].delta.content\n",
    "        print(output, end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
